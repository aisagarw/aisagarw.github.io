<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Aishwarya Agarwal</title>
  <meta name="author" content="Aishwarya Agarwal">
  <meta property="og:url" content="http://aisagarw.me" />
  <meta property="og:title" content="Aishwarya Agarwal" />
  <meta property="og:image" content="http://aisagarw.me/img/Aishwarya.jpg" />

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'EB Garamond', serif;
      background-color: #f9f9f9;
      color: #333;
    }

    .main-card {
      max-width: 900px;
      margin: auto;
      margin-top: 60px;
      background-color: #fff;
      border-radius: 16px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.05);
      overflow: hidden;
    }

    .card-img-top {
      object-fit: cover;
      height: 100%;
      border-bottom: 1px solid #eee;
    }

    .card-body {
      padding: 2rem;
    }

    h1, h2 {
      font-weight: bold;
    }

    h1 {
      font-size: 2.5rem;
    }

    a {
      color: #007bff;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .info-section p {
      margin-bottom: 0.75rem;
    }

    .publications ul {
      list-style-type: none;
      padding-left: 0;
    }

    .publications li {
      margin-bottom: 1.25rem;
    }

    footer {
      text-align: center;
      margin-top: 3rem;
      font-size: 0.9rem;
      color: #777;
    }
  </style>
</head>
<body>



  <div class="main-card">
  <div class="row align-items-center g-0 p-4">
  <div class="col-md-4 text-center mb-3 mb-md-0">
    <img src="img/Aishwarya2.jpg" 
         alt="Aishwarya Agarwal" 
         class="rounded-circle img-thumbnail shadow" 
         style="width: 180px; height: 180px; object-fit: cover;">
  </div>

  <div class="col-md-8">
    <div class="card-body ps-md-4 pe-md-3">
      <h1>Aishwarya Agarwal</h1>
      <p class="lead">Research Associate, Adobe | PhD (Part-time), IIIT Hyderabad</p>

      <p>
        Iâ€™m a Research Associate at <a href="https://research.adobe.com/" target="_blank">Adobe Research</a> India, and a part-time PhD student at CVIT, IIIT Hyderabad. 
        I'm advised by <a href="https://faculty.iiit.ac.in/~vgandhi/" target="_blank">Dr. Vineet Gandhi</a> and co-advised by 
        <a href="https://research.adobe.com/person/srikrishna-karanam/" target="_blank">Dr. Srikrishna Karanam</a>.
      </p>

      <p>
        My research focuses on Representation Learning in low-resource settings, Explainable Computer Vision, and customizing Diffusion Models 
        (e.g., Stable Diffusion) for creative tasks like text-to-image generation.
      </p>

      <p>
        I hold a Dual Degree (BTech + MTech) from <a href="https://www.iitb.ac.in/" target="_blank">IIT Bombay</a> in Electrical Engineering and AI & Data Science. 
        I previously interned at Adobe, working on multimodal understanding and scene enrichment.
      </p>

      <p class="mt-3"><strong>Email:</strong> agarwal.aishwarya2013@gmail.com</p>
      <p><strong>Links:</strong> 
        [<a href="files/Aishwarya_cvl.pdf" target="_blank">CV</a>] 
        [<a href="https://www.linkedin.com/in/aishwarya-agarwal-081a58181/" target="_blank">LinkedIn</a>]
      </p>
    </div>
  </div>
</div>

  <!-- Vertical separator and News -->
<div class="container py-5">
  <div class="row">
    <!-- Vertical Line -->
    <div class="col-md-1 d-none d-md-flex justify-content-center">
      <div style="border-left: 1px solid #ccc; height: 100%;"></div>
    </div>

    <!-- News Section -->
    <div class="col-md-11">
      <h2 class="mb-4">News</h2>
      <ul class="list-unstyled">
       
        <li class="mb-3">
          <strong>[March 28, 2025]</strong> Our work on enabling disentagnled color-style control in diffusion models got accepted in CVPR CVEU workshop 2025!
        </li>

        
        <li class="mb-3">
          <strong>[Feb 27, 2025]</strong> Two papers accepted at <strong>CVPR 2025</strong>. Our paper <b>TIDE</b> on training locally interpretable models is selected as a <b>Highlight</b>! 
        </li>

        <li class="mb-3">
          <strong>[Oct 29, 2024]</strong> Two papers accepted at <strong>WACV 2025</strong>. These works propose training-free methods to customize diffusion models, while mitigating the reconstruction-editability tradeoff.
        </li>

        <li class="mb-3">
          <strong>[Oct 23, 2023]</strong> Our work on using diffusion models for iterative image editing is accepted to <strong>WACV 2024</strong>.
        </li>

        <li class="mb-3">
          <strong>[Jul 14, 2023]</strong> Our work on improving image-text alignment in diffusion models is accepted to <strong>ICCV 2023</strong>!
        </li>

        <li class="mb-3">
          <strong>[Mar 4, 2023]</strong> Our work Sketchbuddy, where we provide automated assistance while sketching, is accepted to ACM MMSys 2023. This was part of my internship project at Adobe during undergrad!
        </li>

        <li class="mb-3">
          <strong>[Mar 4, 2023]</strong> Our work on open-set cross-domain generalization is accepted to WACV 2023. This was part of my thesis at IITB, in collaboration with Adobe Research.
        </li>

        <li class="mb-3">
          <strong>[Mar 4, 2023]</strong> Our work on few-shot class-incremental learning is accepted to ACM Multimedia 2022. My first paper as a lead researcher, all thanks to my advisor Prof. Biplab!
        </li>

        	
      </ul>
    </div>
  </div>
</div>


  

  <div class="container publications mt-5">
    <h2>Publications</h2>

    <h4 class="mt-4">2025</h4>
    <ul>
      <li>
                            <a href="https://arxiv.org/abs/2409.02429" target="_blank">
                                <b>Training-free Color-Style Disentanglement for Constrained Text-to-Image Synthesis</b>
                            </a>
                            <br/>
                            In <a>
                                <b>
                                    Computer Vision and Pattern Recognition Workshop (CVPRW): AI for Creative Visual Content Generation Editing and Understanding</b></a>, 2025.
                            <br/>
			</li>

			<li>
                            <a href="https://arxiv.org/abs/2411.16788" target="_blank">
                                <b>TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction</b>
                            </a>
                            <br/>
                            In <a>
                                <b>
                                    Computer Vision and Pattern Recognition (CVPR)</b></a>, 2025.
                            <br/>
			</li>
			    
                        <li>
                            <a href="https://arxiv.org/abs/2406.10197" target="_blank">
                                <b>Composing Parts for Expressive Object Generation</b>
                            </a>
                            <br/>
                            In <a>
                                <b>
                                    Computer Vision and Pattern Recognition (CVPR)</b></a>, 2025.
                            <br/>
			</li>

			<li>
			    <a href="https://arxiv.org/abs/2311.11919" target="_blank">
                                <b>An Image is Worth Multiple Words: Multi-attribute Inversion for Constrained Text-to-Image Synthesis</b>
                            </a>
                            <br/>

                            In <a>
                                <b>
                                    Winter Conference on Applications of Computer Vision (WACV)</b></a>, 2025.
                            <br/>
                        </li>
                        
                        <li>
			    <a href="https://arxiv.org/abs/2406.18893" target="_blank">
                                <b>AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image Models</b>
                            </a>
                            <br/>

<!-- 			    href="https://wacv2025.thecvf.com/home" target="_blank" -->
                            In <a>
                                <b>
                                    Winter Conference on Applications of Computer Vision (WACV)</b></a>, 2025.
                            <br/>
                        </li>
		    </ul>

		    <h3>2024</h3>
                    <ul class="pl">
                        
                        <li>

			    <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Joseph_Iterative_Multi-Granular_Image_Editing_Using_Diffusion_Models_WACV_2024_paper.pdf" target="_blank">
                                <b>Iterative Multi-Granular Image Editing Using Diffusion Models</b>
                            </a>
                            <br/>

<!-- 			    href="https://wacv2024.thecvf.com/home" target="_blank" -->
                            In <a>
                                <b>
                                    Winter Conference on Applications of Computer Vision (WACV)</b></a>, 2024.
                            <br/>


				
                        </li>
		    </ul>

                    <h3>2023</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Agarwal_A-STAR_Test-time_Attention_Segregation_and_Retention_for_Text-to-image_Synthesis_ICCV_2023_paper.pdf" target="_blank">
                                <b>ASTAR: Test-time Attention Segregation and Retention for Text-to-image Synthesis</b>
                            </a>
                            <br/>
<!-- 			     href="https://iccv2023.thecvf.com/" target="_blank" -->
                            In <a>
                                <b>
                                    International Conference on Computer Vision (ICCV)</b></a>, 2023.
                            <br/>

			</li>
                        <li>
			    <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3590980" target="_blank">
                                <b>SketchBuddy: Context-Aware Sketch Enrichment and Enhancement</b>
                            </a>
                            <br/>

<!-- 			    href="https://2023.acmmmsys.org/" target="_blank" -->
                            In <a>
                                <b>
                                    ACM Multimedia Systems Conference (ACM MMSys)</b></a>, 2023.
                            <br/>

			</li>
                        <li>

			    <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Contrastive_Learning_of_Semantic_Concepts_for_Open-Set_Cross-Domain_Retrieval_WACV_2023_paper.pdf" target="_blank">
                                <b>Contrastive Learning of Semantic Concepts for Open-set Cross-domain Retrieval</b>
                            </a>
                            <br/>

<!-- 			    href="https://wacv2023.thecvf.com/home" target="_blank" -->
                            In <a>
                                <b>
                                    Winter Conference on Applications of Computer Vision (WACV)</b></a>, 2023.
                            <br/>


				
                        </li>
		    </ul>
		    <h3>2022</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548160" target="_blank">
                                <b>Semantics-Driven Generative Replay
for Few-Shot Class Incremental Learning</b>
                            </a>
                            <br/>

<!-- 			    href="https://2022.acmmm.org/" target="_blank" -->
                            In <a>
                                <b>
                                    ACM International Conference on Multimedia (ACM MM)</b></a>, 2022.
                            <br/>
			</li>
		    </ul>
                    <h3>2021</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://aclanthology.org/2021.naacl-main.418/" target="_blank">
                                <b>MIMOQA: Multimodal Input Multimodal Output Question Answering</b>
                            </a>
                            <br/>

<!-- 		            href="https://2021.naacl.org/" target="_blank" -->
                            In <a>
                                <b>
                                    North American Chapter of the Association for Computational Linguistics
                                    (NAACL-HLT)</b></a>, 2021.
                            <br/>
			</li>
    </ul>
  </div>

  <footer>
    &copy; 2025 Aishwarya Agarwal
  </footer>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
