<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Aishwarya Agarwal</title>
  <meta name="author" content="Aishwarya Agarwal">
  <meta property="og:url" content="http://aisagarw.me" />
  <meta property="og:title" content="Aishwarya Agarwal" />
  <meta property="og:image" content="http://aisagarw.me/img/Aishwarya.jpg" />

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'EB Garamond', serif;
      background-color: #f9f9f9;
      color: #333;
    }

    .main-card {
      max-width: 1200px;
      margin: auto;
      margin-top: 60px;
      background-color: #fff;
      border-radius: 16px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.05);
      overflow: hidden;
    }

    .card-img-top {
      object-fit: cover;
      height: 100%;
      border-bottom: 1px solid #eee;
    }

    .card-body {
      padding: 2rem;
    }

    h1, h2 {
      font-weight: bold;
    }

    h1 {
      font-size: 2.5rem;
    }

    a {
      color: #007bff;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .info-section p {
      margin-bottom: 0.75rem;
    }

    .publications ul {
      list-style-type: none;
      padding-left: 0;
    }

    .publications li {
      margin-bottom: 1.25rem;
    }

    footer {
      text-align: center;
      margin-top: 3rem;
      font-size: 0.9rem;
      color: #777;
    }

	  .timeline-date {
  font-weight: 500;
  font-family: 'EB Garamond', serif;
  color: #6c757d;
}

.highlight-text {
  color: #ffbdbd; /* A nice deep indigo/teal tone */
  font-weight: 500;
}
	  

  </style>
</head>
<body>



  <div class="main-card">
  <div class="row align-items-center g-0 p-4">
  <div class="col-md-4 text-center mb-3 mb-md-0">
    <img src="img/Aishwarya2.jpg" 
         alt="Aishwarya Agarwal" 
         class="rounded-circle img-thumbnail shadow" 
         style="width: 200px; height: 200px; object-fit: cover;">
  </div>

  <div class="col-md-8">
    <div class="card-body ps-md-4 pe-md-3">
      <h1>Aishwarya Agarwal</h1>
      <p class="lead">Research Associate, Adobe | PhD (Part-time), IIIT Hyderabad</p>

      <p>
        I’m a Research Associate at <a href="https://research.adobe.com/" target="_blank">Adobe Research</a> India, and a part-time PhD student at CVIT, IIIT Hyderabad. 
        I'm advised by <a href="https://faculty.iiit.ac.in/~vgandhi/" target="_blank">Dr. Vineet Gandhi</a> and co-advised by 
        <a href="https://research.adobe.com/person/srikrishna-karanam/" target="_blank">Dr. Srikrishna Karanam</a>.
      </p>

      <p>
        My research focuses on Representation Learning in low-resource settings, Explainable Computer Vision, and customizing Diffusion Models 
        (e.g., Stable Diffusion) for creative tasks like text-to-image generation.
      </p>

      <p>
        I hold a Dual Degree (BTech + MTech) from <a href="https://www.iitb.ac.in/" target="_blank">IIT Bombay</a> in Electrical Engineering and AI & Data Science. 
        I previously interned at Adobe, working on multimodal understanding and scene enrichment.
      </p>

      <p class="mt-3"><strong>Email:</strong> agarwal.aishwarya2013@gmail.com</p>
      <p><strong>Links:</strong> 
        [<a href="files/Aishwarya_cvl.pdf" target="_blank">CV</a>] 
        [<a href="https://www.linkedin.com/in/aishwarya-agarwal-081a58181/" target="_blank">LinkedIn</a>]
      </p>
    </div>
  </div>
</div>

 <!-- News Section -->
<!-- News Section -->
<div class="container py-5">
  <h2 class="mb-4">News</h2>
  <ul class="list-unstyled">
    <li class="mb-4 d-flex align-items-start">
      <div class="timeline-date pe-4" style="width: 140px;">March 28, 2025</div>
      <div class="flex-grow-1">
        Our work on enabling disentangled color-style control in diffusion models got accepted in <b>CVPR CVEU Workshop 2025</b>!
      </div>
    </li>

    <li class="mb-4 d-flex align-items-start">
      <div class="timeline-date pe-4" style="width: 140px;">Feb 27, 2025</div>
      <div class="flex-grow-1">
        Two papers accepted at <b>CVPR 2025</b>. Our paper <span class="highlight-text">TIDE</span> on training locally interpretable models is selected as a <span class="highlight-text">Highlight</span>! 
      </div>
    </li>

    <li class="mb-4 d-flex align-items-start">
      <div class="timeline-date pe-4" style="width: 140px;">Oct 29, 2024</div>
      <div class="flex-grow-1">
        Two papers accepted at <b>WACV 2025</b> on training-free diffusion model customization while balancing <span class="highlight-text">reconstruction-editability tradeoff</span>.
      </div>
    </li>

    <li class="mb-4 d-flex align-items-start">
      <div class="timeline-date pe-4" style="width: 140px;">Oct 23, 2023</div>
      <div class="flex-grow-1">
        Our iterative image editing work using diffusion models is accepted to <b>WACV 2024</b>.
      </div>
    </li>

    <li class="mb-4 d-flex align-items-start">
      <div class="timeline-date pe-4" style="width: 140px;">Jul 14, 2023</div>
      <div class="flex-grow-1">
        Work on improving <span class="highlight-text">image-text alignment</span> in diffusion models accepted to <b>ICCV 2023</b>!
      </div>
    </li>

    <li class="mb-4 d-flex align-items-start">
      <div class="timeline-date pe-4" style="width: 140px;">Mar 4, 2023</div>
      <div class="flex-grow-1">
        <i>Sketchbuddy</i>, our project on <span class="highlight-text">assisted sketching</span>, is accepted to <b>ACM MMSys 2023</b>. Part of my undergrad internship at Adobe!
      </div>
    </li>

    <li class="mb-4 d-flex align-items-start">
      <div class="timeline-date pe-4" style="width: 140px;">Aug 26, 2022</div>
      <div class="flex-grow-1">
        Research on <span class="highlight-text">open-set cross-domain generalization</span> accepted to <b>WACV 2023</b> — part of my IITB thesis with Adobe.
      </div>
    </li>

    <li class="mb-4 d-flex align-items-start">
      <div class="timeline-date pe-4" style="width: 140px;">June 24, 2022</div>
      <div class="flex-grow-1">
        Our work on <span class="highlight-text">few-shot class-incremental learning</span> accepted to <b>ACM Multimedia 2022</b>. My first lead-author paper with Prof. Biplab!
      </div>
    </li>
  </ul>
</div>



  

  <div class="container publications mt-5">
    <h2>Publications</h2>

    <h4 class="mt-4">2025</h4>
    <ul>
      <li>
                            <a href="https://arxiv.org/abs/2409.02429" target="_blank">
                                <b>Training-free Color-Style Disentanglement for Constrained Text-to-Image Synthesis</b>
                            </a>
                            <br/>
                            In <a>
                                <b>
                                    Computer Vision and Pattern Recognition Workshop (CVPRW): AI for Creative Visual Content Generation Editing and Understanding</b></a>, 2025.
                            <br/>
			</li>

			<li>
                            <a href="https://arxiv.org/abs/2411.16788" target="_blank">
                                <b>TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction</b>
                            </a>
                            <br/>
                            In <a>
                                <b>
                                    Computer Vision and Pattern Recognition (CVPR)</b></a>, 2025.
                            <br/>
			</li>
			    
                        <li>
                            <a href="https://arxiv.org/abs/2406.10197" target="_blank">
                                <b>Composing Parts for Expressive Object Generation</b>
                            </a>
                            <br/>
                            In <a>
                                <b>
                                    Computer Vision and Pattern Recognition (CVPR)</b></a>, 2025.
                            <br/>
			</li>

			<li>
			    <a href="https://arxiv.org/abs/2311.11919" target="_blank">
                                <b>An Image is Worth Multiple Words: Multi-attribute Inversion for Constrained Text-to-Image Synthesis</b>
                            </a>
                            <br/>

                            In <a>
                                <b>
                                    Winter Conference on Applications of Computer Vision (WACV)</b></a>, 2025.
                            <br/>
                        </li>
                        
                        <li>
			    <a href="https://arxiv.org/abs/2406.18893" target="_blank">
                                <b>AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image Models</b>
                            </a>
                            <br/>

<!-- 			    href="https://wacv2025.thecvf.com/home" target="_blank" -->
                            In <a>
                                <b>
                                    Winter Conference on Applications of Computer Vision (WACV)</b></a>, 2025.
                            <br/>
                        </li>
		    </ul>

		    <h3>2024</h3>
                    <ul class="pl">
                        
                        <li>

			    <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Joseph_Iterative_Multi-Granular_Image_Editing_Using_Diffusion_Models_WACV_2024_paper.pdf" target="_blank">
                                <b>Iterative Multi-Granular Image Editing Using Diffusion Models</b>
                            </a>
                            <br/>

<!-- 			    href="https://wacv2024.thecvf.com/home" target="_blank" -->
                            In <a>
                                <b>
                                    Winter Conference on Applications of Computer Vision (WACV)</b></a>, 2024.
                            <br/>


				
                        </li>
		    </ul>

                    <h3>2023</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Agarwal_A-STAR_Test-time_Attention_Segregation_and_Retention_for_Text-to-image_Synthesis_ICCV_2023_paper.pdf" target="_blank">
                                <b>ASTAR: Test-time Attention Segregation and Retention for Text-to-image Synthesis</b>
                            </a>
                            <br/>
<!-- 			     href="https://iccv2023.thecvf.com/" target="_blank" -->
                            In <a>
                                <b>
                                    International Conference on Computer Vision (ICCV)</b></a>, 2023.
                            <br/>

			</li>
                        <li>
			    <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3590980" target="_blank">
                                <b>SketchBuddy: Context-Aware Sketch Enrichment and Enhancement</b>
                            </a>
                            <br/>

<!-- 			    href="https://2023.acmmmsys.org/" target="_blank" -->
                            In <a>
                                <b>
                                    ACM Multimedia Systems Conference (ACM MMSys)</b></a>, 2023.
                            <br/>

			</li>
                        <li>

			    <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Contrastive_Learning_of_Semantic_Concepts_for_Open-Set_Cross-Domain_Retrieval_WACV_2023_paper.pdf" target="_blank">
                                <b>Contrastive Learning of Semantic Concepts for Open-set Cross-domain Retrieval</b>
                            </a>
                            <br/>

<!-- 			    href="https://wacv2023.thecvf.com/home" target="_blank" -->
                            In <a>
                                <b>
                                    Winter Conference on Applications of Computer Vision (WACV)</b></a>, 2023.
                            <br/>


				
                        </li>
		    </ul>
		    <h3>2022</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548160" target="_blank">
                                <b>Semantics-Driven Generative Replay
for Few-Shot Class Incremental Learning</b>
                            </a>
                            <br/>

<!-- 			    href="https://2022.acmmm.org/" target="_blank" -->
                            In <a>
                                <b>
                                    ACM International Conference on Multimedia (ACM MM)</b></a>, 2022.
                            <br/>
			</li>
		    </ul>
                    <h3>2021</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://aclanthology.org/2021.naacl-main.418/" target="_blank">
                                <b>MIMOQA: Multimodal Input Multimodal Output Question Answering</b>
                            </a>
                            <br/>

<!-- 		            href="https://2021.naacl.org/" target="_blank" -->
                            In <a>
                                <b>
                                    North American Chapter of the Association for Computational Linguistics
                                    (NAACL-HLT)</b></a>, 2021.
                            <br/>
			</li>
    </ul>
  </div>

  <footer>
    &copy; 2025 Aishwarya Agarwal
  </footer>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
